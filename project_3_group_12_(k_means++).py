# -*- coding: utf-8 -*-
"""Project-3/Group-12 (K-Means++).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aHY9iPH-opIgPheWnwuxawA1lSxynW-w

***K-Means++ Clustering***
"""

# Name : Prudhviraj Sheela | OSU CWID : A20228857
# Name : Aman Masipeddi | OSU CWID : A20198116  

!pip install pyspark

import pyspark # Importing PySpark
from pyspark import SparkContext 
import numpy as np
import itertools
from operator import add
import random
k = 10 # Number of clusters
I = 20 # Number of Iterations
data_file = '/content/data.txt'
data_content  = np.genfromtxt(data_file) # We initially use genfrmtxt for loading data and get the data into an array
entries,dimensions = data_content.shape # This stores the details about the number of entries and its associated dimensions

sc = SparkContext("local","K-Means++ Algorithm: Clustering algorithm k-means++ style of centroid initialization") # Creating the Spark Context
rdd1 = sc.textFile(data_file) # We read the text file and then preprocess it accordingly
rdd2 = rdd1.map(lambda x : x.split()) # We split the data to collect individual data point as a list
rdd3 = rdd2.map(lambda x : [float(i) for i in x]) # Converting each data point in the list from string to float
rdd4 = rdd3.zipWithIndex().map(lambda x : (x[1],x[0])) # Assiging each data point an index

"""***Squared Eucledian Distance Computation for Initial K-Centroids***"""

# In this below steps we find obtain the cluster centroids associated with K-Means++ accordingly

# The below function computes the squared eucledian distance(D) to existing cluster centroid for finding the farthest centroid data point

def farthest_centroid_computation(a):
  m = np.array(a[1])
  distance_maximum = -np.inf # Assuming the initial maximum distance as "infinity"
  for i in broadcasted_centroids.value:
    j = i[0]
    centroid_point = np.array(i[1])
    squared_eucledian_distance = np.sum((m - centroid_point)**2) # Computing the squared eucledian distance
    if squared_eucledian_distance > distance_maximum: # Here we check which particular data point is the farthest from the choosen cluster's centroid
      distance_maximum = squared_eucledian_distance
      centroid_far = j # Here we update the next centroid details
  return (a[0], distance_maximum) # We return the data point farthest and its maximum distance obtained

# In the below function we filter the centroids based on their values present in the origianl data
def filtering_centroids(a):
  if a[0] in broadcasted_centroids_keys.value: # We check if the choosen original data contains the initial centroid and if it si present we elimiate it
    return ()
  else: # If the above condition is not satisfies then we forward data point 'a' as the new initial centroid
    return a

initial_centroids = sc.parallelize(rdd4.takeSample(False,1)) # We choose the initial centroid and fix it as the first random point
broadcasted_centroids = sc.broadcast(initial_centroids.collect()) # Through this variable we broadcast the initial centroids details
broadcasted_centroids_keys = sc.broadcast(initial_centroids.keys().collect()) # Through this variable we broadcast the initial centroids index which is its key

c = 1 # Maintains the count of clusters obtained in next iterations

# In the below while loop we perform 10 iterations to obtain all the clusters
while c < k:
  filter_data = rdd4.map(filtering_centroids).filter(lambda a : a) # Filters and checks if the intial centroid is in the original data and returns its tuple

  centroids_far_data = filter_data.map(lambda a : farthest_centroid_computation(a)) # Computes for finding the farthest centroid available from the initial centroid

  sum_value = centroids_far_data.map(lambda a : a[1]).sum() # Finding the sum of all dimesnions of the farthest centoid data point

  centroids_far_data = centroids_far_data.map(lambda a : (a[0],a[1]/sum_value)) # Computing the next data centroid by dividing each of its value by obtained sum for evaluating their probabilities

  keys_list = centroids_far_data.keys().collect() # Collecting the index of each next centroid obtained as a list of keys

  values_list = centroids_far_data.values().collect() # Collecting the data points obtained for next centroid obtained as a list of values

  M = np.random.choice(keys_list,1,values_list) # We obtain a random value i.e, the data point from the keys and values obtained 

  centroid_latest = filter_data.filter(lambda a : a[0] == M) # In this we check out if the obtained 'M' data point is equal to that of the filtered data point which is the cluster's centroid

  initial_centroids = initial_centroids.union(centroid_latest) # Now we ass the latest centroid obtained with the initial centroid which is processed with the next iterations of 'M'

  broadcasted_centroids = sc.broadcast(initial_centroids.collect()) # The new cluster's centroids to be broadcasted are reassigned

  broadcasted_centroids_keys = sc.broadcast(initial_centroids.keys().collect()) # The new cluster's keys i.e, the index to be broadcasted are reassigned

  c += 1 # We increment the value of 'c' for obtaining the next cluster's centroid based on the broadcasted centroid details

# Below are the initial random clusters obtained for K-Means++
print("\033[1m" + "K-Centroids obtained in K-Means++.ipynb" + "\033[1m")
print("\n")         
for item in initial_centroids.collect():
  print(item)

"""***Eucledian Distance Computation***"""

# The below function generates the eucledian distance associated between the data point and the centroid
def euclidean_distance_calculation(centroid_data, original_data):
  centroid_number = np.array(centroid_data[1]) # In this we collect the cluster's centroid data point
  datapoint_details = np.array(original_data[1]) # In this we store each data point
  euclidean_distance = np.sqrt(np.sum((centroid_number - datapoint_details)**2)) # Eucledian distance computed is stored in this variable
  return (original_data[0], (centroid_data[0],euclidean_distance)) # Return the (original data point, (associated cluster's centroid, eucledian distance obtained in between the points))

# The below function caluclates the minimum distance obtained for each cluster and assigns those data points to that associated cluster
def minimum_distance(data_row):
  data_index = data_row[0] # Obtaining index of the cluster's centroid
  distance_centroids = data_row[1]  # Getting the list of eucledian distances computed
  centroid_minimum_index = -1 # Initialize the centroid's mimimum index to '-1' initially
  min_distance = -1 # Initialize the minimum distance to '-1' initially
  centroid_closest = None # Initialize the closest centroid value to 'None' initially
  for i in distance_centroids: # In the below iterations we try to obtain the minimum distance for each data point i,e; the centroids
    index_centroid = i[0]
    distance_euclidean = i[1]
    if (distance_euclidean < min_distance) or (min_distance == -1): # In this we check the if the obtained eucledian distance is minimum or not and attach that data point to the centroid closest
      min_distance = distance_euclidean
      centroid_minimum_index = index_centroid
      centroid_closest = (centroid_minimum_index, min_distance)
  return (data_index, centroid_closest) # Returning the data points that are closest to the choosen centroid

# In this function we assign the data points to each associated cluster for which we pass inputs as original data and random obtained centroids
def cluster_assignment(centroids_data, original_data):
  centroid = centroids_data.cartesian(original_data) # Obtaining all the combinations possible with the initial centroids obtained and the original data
  distance_centroids_original_data = centroid.map(lambda a : euclidean_distance_calculation(a[0],a[1])) # Calculating eucledian distance for each combination obtained
  assign_datato_cluster_closest = distance_centroids_original_data.groupByKey().map(lambda a : (a[0],list(a[1]))).map(lambda a : minimum_distance(a)) # Evaluating for the minimum distance that is obtained for computing the cost function
  return assign_datato_cluster_closest # Returning the data points closest to the cluster centroids associated

# In the below functions we compute for the next set of centroids for the next iteration
def computing_centroid(indexof_centroid, elementsof_cluster):
  final_cluster_elements = []
  for item in elementsof_cluster:
    final_cluster_elements.append(item[0])
  array_average = list(np.average(final_cluster_elements, axis = 0)) # Centroid coordinates computation
  centroid_obtained = (indexof_centroid, array_average)
  return centroid_obtained

# Computing new centroids from the previous centroids obtained in the before iterations
def computing_centroids(min_distance):
  cluster_data = min_distance.join(rdd4).map(lambda a : (a[1][0][0],(a[1][1],a[1][0][1])))
  cluster_data = cluster_data.groupByKey().map(lambda a : (a[0],list(a[1])))
  centroids_obtained = cluster_data.map(lambda a : computing_centroid(a[0],a[1]))
  return centroids_obtained

iter = 0 # This variable is to update the number of iterations
final_res = np.zeros((I,2)) # This variable stores the final eucledian cost obtained for each iteration
gen_centroids = initial_centroids # These are initial centroids obtained in the above step
cost_eucledian = dict() # This variable stores the eucledian cost of each iteration obtaine for computing the percentages of cost function associated for eucledian metric

# In the below step we iterate the algorithm through I=20 iterations for evaluating the distance metrics and their associated cost functions
while iter < I:

  iter += 1 # Incrementing each iteration

  assign_datato_cluster = cluster_assignment(gen_centroids, rdd4) # Obtaining the details of data points assigned for all the clusters associated

  centroids_new_computed = computing_centroids(assign_datato_cluster) # Computing for the next set of cluster's centroids for the next set of iterations

  clusters_assigned = assign_datato_cluster.map(lambda a : a[1][1]**2) # Collecting all the centroids assigned for each cluster and finding their squared distance

  costfun_eucledian = clusters_assigned.sum() # We find the cost function for each iteration by summing all the centroid points for the associated clusters

  print("Iteration Number %.1d : %.2f" %(iter,costfun_eucledian)) # We print the output of costfun obtained at each iteration

  cost_eucledian[iter] = costfun_eucledian # Stores the eucledian distance obtained for each iteration for computing its percentage and measure performances

  final_res[iter-1,:] = np.array([iter, costfun_eucledian]) # In this we store the iteration and its corresponding cost function obtaine for plotting the graph in successive steps

  gen_centroids = sc.parallelize(centroids_new_computed.collect()) # This stores the next set of centroids for the next iterations and again the process continues accordingly

# Below packages are necessary for creating the line plots
import matplotlib as mplib
import matplotlib.pyplot as pl
figure1, axes1 = pl.subplots(1,1,figsize=(7,5)) # Specifying the dimensions of graph
# In the below step we assign a line plot for the iterations and the cost function
axes1.plot(final_res[:,0], final_res[:,1], 'go-', linewidth = 2, markersize = 12, markeredgewidth = 2, label = 'Cost Function Normalized', fillstyle = 'none')
axes1.set_xlabel("$No$ of $Iterations$ $(I)$") # Setting the x-label to no.of iterations
axes1.set_ylabel("$Cost$ Function $(Φ)$") # Setting the y-label to cost function of eucledian
axes1.set_title("$K-$Means $Clustering$") # Assigning the title for the graph
axes1.legend(loc=0)
figure1.tight_layout() # Adjusts the graph to a tight layout
pl.show() # Displays the plot

"""***Manhattan Distance Computation***"""

# The below function generates the manhattan distance associated between the data point and the centroid
def manhattan_distance_calculation(centroid_data, original_data):
  centroid_number = np.array(centroid_data[1]) # In this we collect the cluster's centroid data point
  datapoint_details = np.array(original_data[1]) # In this we store each data point
  manhattan_distance = np.sqrt(np.sum(np.abs(centroid_number - datapoint_details))) # Manhattan distance computed is stored in this variable
  return (original_data[0], (centroid_data[0],manhattan_distance)) # Return the (original data point, (associated cluster's centroid, manhattan distance obtained in between the points))

# The below function caluclates the minimum distance obtained for each cluster and assigns those data points to that associated cluster
def minimum1_distance(data_row):
  data_index = data_row[0] # Obtaining index of the cluster's centroid
  distance_centroids = data_row[1] # Getting the list of eucledian distances computed
  centroid_minimum_index = -1 # Initialize the centroid's mimimum index to '-1' initially
  min_distance = -1 # Initialize the minimum distance to '-1' initially
  centroid_closest = None # Initialize the closest centroid value to 'None' initially
  for i in distance_centroids: # In the below iterations we try to obtain the minimum distance for each data point i,e; the centroids
    index_centroid = i[0]
    distance_manhattan = i[1]
    if (distance_manhattan < min_distance) or (min_distance == -1): # In this we check the if the obtained eucledian distance is minimum or not and attach that data point to the centroid closest
      min_distance = distance_manhattan
      centroid_minimum_index = index_centroid
      centroid_closest = (centroid_minimum_index, min_distance)
  return (data_index, centroid_closest) # Returning the data points that are closest to the choosen centroid

# In this function we assign the data points to each associated cluster for which we pass inputs as original data and random obtained centroids
def cluster1_assignment(centroids_data, original_data):
  centroid = centroids_data.cartesian(original_data) # Obtaining all the combinations possible with the initial centroids obtained and the original data
  distance_centroids_original_data = centroid.map(lambda a : manhattan_distance_calculation(a[0],a[1]))  # Calculating manhattan distance for each combination obtained
  assign_datato_cluster_closest = distance_centroids_original_data.groupByKey().map(lambda a : (a[0],list(a[1]))).map(lambda a : minimum1_distance(a)) # Evaluating for the minimum distance that is obtained for computing the cost function
  return assign_datato_cluster_closest # Returning the data points closest to the cluster centroids associated

# In the below function we compute for the next set of centroids for the next iteration
def computing1_centroid(indexof_centroid, elementsof_cluster):
  final_cluster_elements = []
  for item in elementsof_cluster:
    final_cluster_elements.append(item[0])
  array_average = list(np.average(final_cluster_elements, axis = 0)) # Centroid coordinates computation
  centroid_obtained = (indexof_centroid, array_average)
  return centroid_obtained

# Computing new centroids from the previous centroids obtained in the before iterations
def computing1_centroids(min_distance):
  cluster_data = min_distance.join(rdd4).map(lambda a : (a[1][0][0],(a[1][1],a[1][0][1])))
  cluster_data = cluster_data.groupByKey().map(lambda a : (a[0],list(a[1])))
  centroids_obtained = cluster_data.map(lambda a : computing1_centroid(a[0],a[1]))
  return centroids_obtained

iter1 = 0 # This variable is to update the number of iterations
final_res1 = np.zeros((I,2)) # This variable stores the final manhattan cost obtained for each iteration
gen_centroids1 = initial_centroids # These are initial centroids obtained in the above step
cost_manhattan = dict() # This variable stores the eucledian cost of each iteration obtaine for computing the percentages of cost function associated for manhattan metric

# In the below step we iterate the algorithm through I=20 iterations for evaluating the distance metrics and their associated cost functions
while iter1 < I:

  iter1 += 1 # Incrementing each iteration

  assign_datato_cluster1 = cluster1_assignment(gen_centroids1, rdd4) # Obtaining the details of data points assigned for all the clusters associated

  centroids_new_computed1 = computing1_centroids(assign_datato_cluster1) # Computing for the next set of cluster's centroids for the next set of iterations

  clusters_assigned1 = assign_datato_cluster1.map(lambda a : a[1][1]**2) # Collecting all the centroids assigned for each cluster and finding their manhattan cost function distance

  costfun_manhattan = clusters_assigned1.sum()  # We find the cost function for each iteration by summing all the centroid points for the associated clusters

  print("Iteration Number %.1d : %.2f" %(iter1,costfun_manhattan)) # We print the output of costfun obtained at each iteration

  cost_manhattan[iter1] = costfun_manhattan # Stores the manhattan distance obtained for each iteration for computing its percentage and measure performances

  final_res1[iter1-1,:] = np.array([iter1, costfun_manhattan]) # In this we store the iteration and its corresponding cost function obtaine for plotting the graph in successive steps

  gen_centroids1 = sc.parallelize(centroids_new_computed1.collect()) # This stores the next set of centroids for the next iterations and again the process continues accordingly

figure2, axes2 = pl.subplots(1,1,figsize=(7,5)) # Specifying the dimensions of graph
# In the below step we assign a line plot for the iterations and the cost function
axes2.plot(final_res1[:,0], final_res1[:,1], 'yo-', linewidth = 2, markersize = 12, markeredgewidth = 2, label = 'Cost Function Normalized', fillstyle = 'none')
axes2.set_xlabel("$No$ of $Iterations$ $(I)$") # Setting the x-label to no.of iterations
axes2.set_ylabel("$Cost$ Function $(Ψ)$") # Setting the y-label to cost function of manhattan
axes2.set_title("$K-$Means $Clustering$") # Setting the title
axes2.legend(loc=0) 
figure2.tight_layout() # Adjusts the graph to a tight layout
pl.show() # Displays the plot

print(cost_eucledian) # Obtained cost function eucledian distances for each iteration
print(cost_manhattan) # Obtained cost function manhattan distances for each iteration

"""***Comparing Performances of Cost Functions between Eucledian and Manhattan in K-Means++:***

From the graphs plotted for both the cost functions of Eucledian and Manhattan it is observable that the cost values obtained for each iteration for a Eucledian function have higher associated values and they decrease in a exponential manner.

On observing the cost values obtained for each iteration for Manhattan function have lower associated values and they decrease in an exponential manner with some slight changes at a few iterations and later they decrease for the next set of iterations.

So, we can observe that Eucledian distance measurement has higher performance than the Manhattan distance measurement since the data considered is obtained for lower dimensional vector space.

***Cost Function Percentages Computation***
"""

# Computing the cost percentage for eucledian distance metric
cost_percentage_eucledian = ((cost_eucledian[1] - cost_eucledian[10])/(cost_eucledian[1])) * 100
cost_percentage_eucledian

# Computing the cost percentage for manhattan distance metric
cost_percentage_manhattan = ((cost_manhattan[1] - cost_manhattan[10])/(cost_manhattan[1])) * 100
cost_percentage_manhattan

"""***Cost Functions Percentage in K-Means.ipynb:***

a) Eucledian: 47.91588106137593

b) Manhattan: 25.728879468765054

***Cost Functions Percentage in K-Means++.ipynb:***

a) Eucledian: 66.55113408072395

b) Manhattan: 43.16135235268827

***Comparing performances between K-Means and K-Means++:***

--> As we can see the results generated for both K-Means and K-Means++ in terms of their cost function percentages evaluated, it is clearly observable that K-Means++ has better performance than K-Means.

--> The one reason this could take place is since the K-Means++ centroids are distributed over the data it is more likely to have reduced cost compared to that of K-Means where its centroids are randomly initialized.

--> Also as the centroids are randomly initialized in K-Means it can generate different set of results based on the initial set of centroids choosen. So, in order to improve this the one solution that could help in increasing the performance of K-Means algorithm is to choose the initial centroids based on iterations but not in a random fashion.

***Team Members Contribution:***

***Prudhviraj Sheela***

Worked on finding the computation parts of obtaining the initial set of clusters in both K-Means and K-Means++, in calculating the eucledian distance for both the algorithms and plotted the necessary line plot graphs for the eucledian cost functions.

***Aman Masipeddi***

Worked on finding the computation parts of manhattan distance and plotted the necessary line plot graphs for the manhattan cost functions in both K-Means and K-Means++ programs. Also worked on computing the cost percentage functions for both eucledian and manhattan in both the programs.


As a whole we both discussed different points on how to compare the performances obtained for the cost functions percentages in both the programs and we had written the responses for it accordingly.
"""